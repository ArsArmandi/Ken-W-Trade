services:
  llama:
    build: .
    ports:
      - "8080:8080"
    volumes:
      - ~/.cache/llama:/models:z
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    group_add:
      - video
      - render
    environment:
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - ROCR_VISIBLE_DEVICES=0
    security_opt:
      - label=disable
    command: ["--host", "0.0.0.0", "--port", "8080", "-m", "/models/GLM-4.7-Flash-Uncen-Hrt-NEO-CODE-MAX-imat-D_AU-IQ3_M.gguf", "--ctx-size", "128000", "--flash-attn", "on"]
